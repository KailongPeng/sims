Today I am going to give a short overview of two completed projects and one ongoing project using different method like image synthesis and real-time neurofeedback and simulation to study the learning and learned brain.
Neural network models are by far the best model of the brain. This framework aims to build a cleaner mapping between any network that can synthesize images and the brain.




The method we are using to build the mapping is representational similarity analysis (RSA).
RSA has been widely used to map between different systems by comparing their activation patterns.
By feeding two images into the model and the brain, we can obtain activation patterns corresponding to these input images. 
Correlating these activation patterns gives us this representational similarity matrix. Say we feed 16 images to the model or the brain, we can obtain a 16 by 16 correlation matrix. This representational similarity matrix represents the activity pattern of the selected model layer, or we can also call this matrix the fingerprint of this model layer or brain region. 
Feeding the same 16 images to the brain would give us the fingerprint of a selected brain region,
By comparing the fingerprint of the model layer and brain region, the similarity level of these two systems can be quantified.

Normally, RSA would use natural or abstract images to map between different systems, 
⭐️This would result in a less clear mapping between the two systems. As you can see here, none of them shows a crystal clear mapping like this ⭐️
This is because closer layers would elicit more similar RSM fingerprints, thus creating more overlapping mapping between the two systems. 
The current goal of this project is to make the mapping statistically less overlapping and more clean.

The image synthesis framework is like this,
⭐️We first feed in a set of 16 randomly initiated images into the ⭐️ Inception CNN model, then ⭐️ for example for layer 10, we correlate the activation patterns of these 16 images and obtain this 16 by 16 representational correlation matrix or fingerprint. ⭐️ We do the same thing for all other 11 layers and obtain 12 fingerprints for 12 model layers. Since our image synthesis goal is to minimize the similarity between these fingerprints of model layers, we can correlate these model layer fingerprints with each other and minimize the sum of their correlation. For example, ⭐️if we correlate layer 1 and layer 2 fingerprints, we obtain one value of this matrix. Now if we square and sum this second-order correlation matrix, we would be able to minimize the similarity between different model layer fingerprints.
⭐️ Based on this goal, we update the initial random images, and ⭐️finally obtain this set of 16 images.

⭐️As a control, we obtain 5000 sets of 16 images from the Natural scene dataset (NSD) (NSD is a public fMRI dataset recording participants’ brain activity while they view 5000 natural scene images) and ⭐️ calculate the baseline similarity between model layer fingerprints, here as you can see the mean second-order matrix of these 5000 sets of NSD images and they are pretty high, meaning that the baseline similarity between layer fingerprints is high. 
⭐️For the 5000 sets of NSD images, we also did some statistical analysis of the second-order matrix. We resampled these 5000 second order correlation matrix means to obtain this grey distribution, and found that the synthesized images are much smaller than the grey null distribution, indicating that the synthesized images are doing a better job reducing the similarity between fingerprints of different model layers.

After synthesizing these images, we next collected data from participants viewing synthesized images.
⭐️Up to 8 runs of fMRI data collected from 30 participants

⭐️In each run, 16 images were shown 5 times each in random order designed to deconvolve image-specific BOLD responses

⭐️ The behavioral task was to fixate on the center of the screen and report the detection of a gray square that appeared occasionally on the image using a button box.

The collected data was then fitted with GLM to extract the activation patterns of each image separately for each subject.


In order to build the mapping between the brain and model, we ran a searchlight across the brain to localize different model layers in the brain.
We used leave one subject out manner to train a linear model and check whether the model is significant across subjects to localize model layers in the brain.



Fig 2. RSA and linear regression to map between brain and model.
Leave one subject out linear regression between RSMs of model layer and brain area is used to map between brain and model.
After we obtained the activation patterns of synthesized images, we then fitted a leave-one-subject-out linear regression in a searchlight manner, where the searchlight carved up the brain into a lot of voxel cubes and ran the pattern analysis in all of them.

⭐️In this leave one subject out linear regression, each model layer is processed separately. Here for illustration, we are dealing with layer 2.
We fitted a linear model between the fingerprint of model layer 2 and of the brain region. 
⭐️Similar model is fitted for subject 2,
⭐️We then obtain the average of the fitted models and use the mean model to ⭐️test how much this model can explain the left-out subject’s data, that is R square. ⭐️This R square is normalized with a shuffled model to obtain a ⭐️ z-scored R square value between model layer 2 and the current brain region.

⭐️After full leave one subject out rotation and searchlight across the whole brain, the n normalized R square volumes are summarized with randomise TFCE statistics.
Here is the visualization result of the previous analysis method for the synthesized images.
Our goal is to quantify whether synthesized images reduce the overlap degree of different model layers in the brain compared with baseline, so we designed an overlap metric.




Fig 3. Localization in the brain for the synthesized images using leave-one-out linear regression.
The set of 16 synthesized images was presented to both human participants and a CNN model. Spatial patterns of activity corresponding to each of the 16 images were extracted from each searchlight position in the brain, and from each of the selected layers in the model. Next, within each region or layer, we computed the representational similarity matrix (RSM) across all 16 images. Finally, we fit a multiple linear regression model predicting brain RSMs from each model layer RSM, using the data from all but one held out participant. We then tested this model on the held out participant. The results of our leave-one-out regression analysis predicting brain RSMs from model layer RSMs for synthesized images. 
A posterior view of the whole-brain results for each individual model layer, summarized using TFCE with a significance threshold set at 0.95.
⭐️This overlap metric quantifies how much different model layers in the brain overlap with each other. The higher this metric is the more overlap different layers are.
⭐️This is the overlap matrix for the synthesized images. We want to compare this matrix with the control overlap matrix.



Fig 5. Overlap for the synthesized images in the brain
a. Two circles were utilized to symbolize two distinct brain areas corresponding to two model layers M and N. m represented voxels exclusively associated with layer-M, and mn represented voxels shared by both layer-M and layer-N. n represented voxels unique to layer-N. The overlap metric was defined as the ratio of shared voxels mn divided by the total number of voxels corresponding to layer-M: mn/(mn+m)            
b. A matrix was presented illustrating the overlap metrics between 12 layers. The y-axis of the matrix represented the denominator or layer M. The majority of pairs of layers exhibited minimal overlap in comparison to natural images.
For the 16 set of 16 images selected from NSD images, we repeat the same analysis introduced above for the synthesized images and obtain this mapping for natural images.



Fig 4. Localization in the brain for NSD images using  using leave-one-out linear regression.}
16 sets of 16 images randomly sampled from NSD were presented to subjects and the CNN model. The RSM of the brain and model layers are compared with each other in a leave-one-subject-out manner to map between model layers and the whole brain with the searchlight method. The result of searchlight is averaged across sets are then fed into randomise TFCE with a significance threshold set at 0.95.
So we repeat the leave one subject out linear regression in the NSD dataset, which uses natural scene images. We randomly select 16 sets of 16 images from NSD,
⭐️this is the overlap metric matrix averaged across 16 sets. 
⭐️To compare this natural image overlap baseline with our synthesized image result, we resample across the 16 matrix mean value to obtain a distribution of the NSD overlap metric, which is this grey distribution. The red line is the synthesized image overlap metric result. It turns out that the synthesized images significantly reduce overlap between layers in the brain compared with the control NSD images.






Fig 6. Overlap analysis for NSD images with non-residual method
a. A 12 by 12 overlap matrix for each set of NSD non-residual results was obtained. To visualize the collective pattern across the 16 sets, these overlap matrices were averaged, resulting in a representative matrix as shown.            
b. To examine the overlap between all layers, the upper triangle of the 12 by 12 matrix was focused, as indicated by the green inset. To compare the overlap results between NSD residual results and synthesized results, we conducted 5000 iterations of resampling. During each iteration, we calculated the mean of the upper triangle of the overlap matrices obtained from the 16 sets of NSD non-residual results. This generated a histogram that provided insights into the statistical characteristics of the overlap values. As a comparison, a red line representing the mean of the upper triangle of the overlap matrix obtained from the synthesized image results was plotted.            
c. The overlap results between neighboring layers were also investigated. Similar to the analysis in point b, the off-diagonal elements of the matrix were focused, as shown in the green inset.            The findings revealed that the non-residual method applied to NSD images yielded statistically significantly larger overlap compared with synthesized results.
Other than directly synthesizing images to reduce overlap, there is another method that can be used to reduce overlap between layers in the brain, which is the residual method.
In linear regression, instead of fitting between the fingerprint of a single model layer and a brain region, we can also fit linear regression between multiple model layers and a brain region.

⭐️This is what I just showed, which is linear regression between the fingerprint of a single layer and of a brain region
⭐️Alternatively, we can do linear regression between fingerprints of all but the target layer
⭐️We then remove the explained variance of all but one layer to obtain the residual variance
⭐️We can fit another linear regression between the residual variance and the target layer fingerprint
⭐️This will make sure that only the brain region that is specific to the target layer will show up and thus reduce overlap.

Now we modify the original pipeline by replacing the leave one subject out linear regression to leave one subject out residual linear regression, and obtain the overlap matrix for this residual method.

We add this residual method into the leave one subject out cross-validation pipeline to obtain a less overlapping brain-to-model mapping. 
⭐️We first obtain the residual variance of one linear regression model and then fit a linear regression between the residual variance and the target layer fingerprint. 
⭐️The same thing is done for subject 2,
⭐️Same as before, this model is averaged across n-1 participants and obtain the ⭐️R squared on the left out subject. The R square is ⭐️normalized with shuffling method⭐️.
⭐️After full rotation of the leave one subject out method, we have n normalized R square volumes. They are fed into randomise for visualization.





In this leave one subject out linear regression, each model layer is processed separately. Like here we are dealing with layer 2.
We fitted linear model between model layer2 fingerprint and the brain region. We then obtain the average of the fitted models and use the mean model to test how much this model can explain the left out subject’s data, that is R square. This R square is normalized with a shuffled model to obtain a z-scored R square value between model layer 2 and the current brain region.

After full leave one subject out rotation and searchlight across the whole brain, the n normalized R square volumes are summarized with randomise TFCE statistics.

Here is the visualization of the residual model.



Fig 7. Localization in the brain for NSD images with residual method.
16 sets of 16 images randomly sampled from NSD were presented to subjects and the CNN model.             To establish a mapping between the RSMs of the brain and model layers, we employed a leave-one-subject-out approach. During this process, The unique variance explained by a single model layer was extracted by excluding the explained variance of all other model layers. Utilizing a searchlight method, we mapped the relationships between model layers and the entire brain.             The result of searchlight is averaged across sets are then fed into randomise TFCE with a significance threshold set at 0.95.
⭐️ Here is the overlap metric matrix for the residual method, we can also compare the overlap matrix of residual method with the synthesized image results.
⭐️ It turns out that the residual method applied to NSD images would elicit an equal amount of overlap as the synthesized images both for all layers and
⭐️ for neighboring model layers.


fig8
⭐️Synthesized images can help us reveal more precise non-overlapping mapping between the model and the brain.
⭐️The residual method can build a comparable level of non-overlapping mapping.
⭐️The synthesis and residual approaches can be used for different purposes. 
If we're starting a new study, it might be more efficient to synthesize images. We might need fewer stimuli to optimize the stimuli. 
But if we're working with existing data like NSD, we take this residual approach to find more selective model layer correspondence in the brain.



project 1: 12 min















These two methods also follow different philosophies of image selection. The image synthesis approach captures the parts of representation manifolds that undergo the most drastic changes between layers and the residual method randomly selects images on the representational manifolds.
Here is a manifold interpretation simulation of the image synthesis process. 
⭐️Here we use an S-shaped manifold to represent layer 1 manifold and a flat sheet to represent the layer 2 manifold. We initialize the images randomly so that the dots are randomly distributed across the manifold as shown in the upper half panel a and b.
⭐️After image synthesis with the target of minimizing the layer fingerprint similarity. We found that the dots start to tile the parts of the manifold that undergo the most drastic changes.
So we interpret the two methods as two distinct principles, for the image synthesis method, it is tiling up the most drastically changing parts of the manifold. And for the NSD images residual method, it is randomly tiling up the whole representation space.





64 dots are randomly selected in layer 1 S-shape representational manifold. 
(b) the same 64 dots as a are visualized in layer 2 unrolled representational manifold. 
(c) After optimization that is similar to the image synthesis process, the 64 dots are visualized in layer 1 representational manifold.             
(d) The same dots as c are visualized in layer 2 representational manifold. % Colorbar is used only for the purpose of visually comparing the representational manifolds of layer 1 and layer 2.            This toy dataset visualization gives us intuition about our image synthesis process, which is that our image synthesis objective drives the synthesized images to find out the manifold parts that undergo the most drastic changes.
The second project tries to use real-time neurofeedback to induce plasticity in the hippocampus.
Our brain partially re-uses neurons when forming different memories, namely, different memories have distributed representations in the brain. 
For example, ⭐️when we view a bed, it can also activate ⭐️ some competitor object that partially shares the bed representation, like a chair. 
⭐️Such coactivation can induce learning, which affects how the target and competitor memories are subsequently represented. 
Both ⭐️ integration and ⭐️ differentiation have been reported in previous fMRI studies. To reconcile these mixed findings, The work from our lab has proposed non-monotonic plasticity hypothesis suggests that when two representations are ⭐️highly coactivated, this leads to integration; 
when two representations are ⭐️moderately coactivated, this leads to differentiation.




The current study tries to provide a more direct test of how coactivation drives representational change in the hippocampus.

We designed an experimental system that can ⭐️ collect and ⭐️process the fMRI data in real-time 
and then ⭐️ present the result back to the subject, thus creating a closed loop from the brain to the brain. 
This allows us to ⭐️ change the task online based on brain states, 
⭐️ Measures the whole brain simultaneously, 
with⭐️ good spatial resolution.

We collected the data like this.
For each subject, there are 5 sessions in total. Three of them are feedback sessions and the beginning and ending sessions are recognition sessions. 
This is a very large neurofeedback fMRI dataset collected across 5 sessions for 20 participants.



Table 1: Multi-session study protocol for each participant.
The overall goal is to use real-time neurofeedback to increase the ⭐️co-activation of bed and chair, thus inducing the integration between bed and chair representation. ⭐️We also have table and bench as control. 
⭐️We first show these 4 types of objects to participants in the recognition runs to record their representations of these four objects. 







(a) Each participant received two objects for neurofeedback and the other two objects served as a baseline. 
(b) The presented object (e.g., bed) was shown on neurofeedback trials and began oscillating in size and shape. The goal of the participant was to make this wobbling stop, which they could achieve by activating the representation of the competitor object (e.g., chair) in their mind. Evidence for the competitor object was quantified based on a classifier trained to decode the competitor object relative to two control objects (e.g., table, bench). The amount of classifier evidence for the competitor needed to reduce the magnitude of wobbling was staircased to maximize coactivation between these objects. 
(c) Recognition trials showed one of the four objects at a time with no neurofeedback. These trials were used to train the classifier models and to measure neural snapshots of how the object is presented in the hippocampus.
(d) Feedback trials were collected during real-time fMRI in order to induce coactivation. The main neurofeedback occurred during the object presentation (the amount of wobbling), though participants also received a valenced emoji based on their performance relative to their staircase.
⭐️Then we trained two classifiers between table and chair and between bench and chair, 
⭐️ These two classifiers are used in the neurofeedback run to drive the subject to think more about the chair. 
⭐️The participants were shown a wobbling bed image, the current brain state was recorded in real-time and pushed towards the chair by the neurofeedback loop.
⭐️Here is the illustration of the neurofeedback trial, the participants were shown a wobbling bed. When the subject is doing a good job thinking more about the chair, the wobbling degree is reduced, and a smiley face and monetary feedback is offered.



⭐️Here are the ROIs we used to drive neurofeedback for different participants, which is defined in a data-driven way to find out the best combination of ROIs that can distinguish between four objects. 

Since this is the first time neurofeedback is used for co-activation activation, we want to test whether real-time neurofeedback driving is successful. 
⭐️ In the neurofeedback task, we change how hard the task is based on participants’ performance in recent trials. The difficulty of the task is determined by an adaptive threshold. This is an example curve of threshold for one subject, showing that increases a lot, meaning that the participant is doing a better job. 
We did a linear regression between the threshold and session ID and found this threshold is increasing significantly across different feedback sessions, indicating that the participants are more and more capable of activating the chair representation and obtaining better performance. This is more like the behavioral evidence of successful co-activation.
⭐️ We also recorded the co-activation level of the presented object bed and competitor object chair. 
⭐️ A linear regression is also done here and an increase across feedback sessions in the coactivation is also found.







We find that the subject successfully increase their driving level in the experiment.

When looking at the 

(a) The mega ROI used to provide neurofeedback was customized for each participant in a data-driven manner. This plot shows the number of participants whose feedback ROIs contained each voxel. The greatest consistency (virtually every participant) was observed in the early visual cortex, extending ventral and lateral. 
(b) Participants succeeded in self-generating classifier evidence for the competitor object based on neurofeedback, as indicated by an increase across the feedback sessions in the threshold from the staircasing procedure. A higher threshold over time indicates that participants were able to activate the competitor object more and more strongly. 
(c) As a corollary of this effect, we also found an increase across feedback sessions in the coactivation between the presented and competitor objects, quantified as the product of the classifier evidences for these objects relative to the control object baseline. ∗∗ = p < 0.01, ∗ = p < 0.05.
After we have shown that real-time feedback successfully drives co-activation, we want to know whether co-activation leads to behavioral and neural changes. Here is the behavioral effect.
We collected some behavioral data before and after the whole experiment. 
⭐️In this categorical perception task, participants were shown a morph from a continuum between two objects and asked to make a forced alternative choice of what they perceived. 
⭐️Subjects’ behavioral categorization responses and the morphing level can be modeled with logistic regression, the steeper the logistic regression slope is, the more capable the subject is of discriminating along the axis. ⭐️
For each participant, Two models were fit to the first session ⭐️ and last session ⭐️, the change of slope for the trained and untrained axis is compared. If the change in the trained axis is bigger, that means integration happens in the trained axis. 
⭐️Here are the logistic regression fits for both trained and untrained axes
⭐️Here the change in slope is bigger than 0 for the trained axis while not bigger than 0 for the untrained axis, meaning that after neurofeedback, behavioral integration appears.
After looking at the behavioral integration effect, we now look at the neural integration effect. Specifically, we now look at the hippocampal integration effect.
⭐️The hippocampus is a critical brain structure for storing memories and needs to resolve competition between memories to avoid interference. 
⭐️The hippocampus contains several subfields with different functional roles that we can segment from anatomical scans using automated methods. 
⭐️In a previous study in our lab, we synthesized images from a neural network that varied the degree of coactivation between memories. 
We found that moderate co-activation of two memories resulted in memory differentiation in DG. 
⭐️Similar patterns are found in other hippocampal subfields.
However, there was no clear evidence of integration.
Now in this neurofeedback study, since we are driving the co-activation as high as possible, whether integration can be tested in the hippocampus or any of its subfields.





, suggesting that we may have only sampled part of the non-monotonic curve. This creates an opportunity to manipulate co-activation more across the ⭐️ full spectrum.


For the hippocampus and subfield analysis, we also trained a classifier between ⭐️ the presented object chair and the competitor object, the testing accuracy before neurofeedback training and after neurofeedback training is compared to indicate the integration. The larger the decrease in accuracy, the less capable the subject is differentiating these two objects, the more integrated these two objects are.
⭐️This is done in both the trained and untrained axis and used the classifier’s testing accuracy to define neural integration. 
⭐️We found that in the hippocampus and its subfields CA1 and in the subiculum, we successfully induced integration. This is consistent with prior work showing that CA1 plays a role in integration. 




DG also shows integration in the trained axis, which is not as predicted that differentiation often happens in DG, potentially showing that the neurofeedback driving is strong enough to drive DG to integration.

The subiculum is a structure traditionally considered a relay from the hippocampus to other brain regions. Indeed, the subiculum is a crucial link to the brain’s reward system, which was likely engaged by the neurofeedback received







The resulting classification accuracy in session 1 and session 5 is compared and the change of accuracy was used to define neural integration. The larger this change is, the more integration happens.




Figure 4. Representational change in the hippocampus. 
(a) We collected neural snapshots of how the four objects were represented in each ROI before (Session 1) and after (Session 5) neurofeedback training. Participants viewed the objects (intact, no wobbling or morphing) during a recognition task in these sessions. For each participant, we built regularized logistic regression classifiers to discriminate multivoxel patterns of BOLD activity in each ROI evoked by the presented vs. competitor objects (trained axis) and by the control 1 vs. 2 objects (untrained axis). Using leave-one-run-out cross-validation, we quantified the neural overlap along these two axes from the test classifier accuracy (high accuracy indicates less overlap). A neural integration index was defined for each ROI and axis as classifier accuracy in Session 1 minus Session 5 (positive reflects a decrease in classifier accuracy). 
(b) In the whole hippocampus, there was reliable neural integration along the trained axis relative to the untrained axis. The integration for the trained but not untrained axis was reliably above chance. 
(c) The difference in neural integration between trained and untrained axes was reflected most clearly in the CA1 subfield, though also marginally in the subiculum, and the trained axis was above chance in the dentate gyrus. The insets depict the location of the ROI in red on an example participant’s T2 scan. Each dot in the bar plots represents an individual participant and the error bands reflect the 95% confidence interval from bootstrap resampling. ∗ = p < 0.05.
Given that we observed both behavioral and neural integration after neurofeedback training, we next asked if these effects are related. Namely, does the increased neural overlap of the trained objects in the hippocampus, CA1, and PHC have behavioral significance for categorical perception? 
⭐️For the trained axis, we found a marginal positive correlation between neural integration in the hippocampus and behavioral integration (r = 0.383, p = 0.095). ⭐️This brain-behavior relationship was a bit clearer in PHC (r = 0.445, p = 0.049), but ⭐️not found in CA1 (r = 0.020, p = 0.936). 
As a control, we repeated this analysis for the untrained axis and found no reliable correlations (ps > 0.403).



The behavioral integration effect should correlate with the neural integration effect. So we fitted a linear regression between them ⭐️ and found in parahippocampal cortex, the neural and behavioral integration effect have a significant linear relationship. The hippocampus has a marginal effect.




Figure 5. Behavioral relevance of neural representational change 
The amount of neural integration predicted the amount of behavioral integration in PHC and marginally in the hippocampus, though not reliably in CA1. Each dot represents an individual participant and the line indicates the fitted linear relationship. ∗ = p < 0.05, = p < 0.10.
⭐️Across multiple real-time fMRI training sessions, participants succeeded in using the neurofeedback to induce coactivation of two representations. 

⭐️Compared with untrained objects, this coactivation led to behavioral and neural integration: The trained objects became harder for participants to discriminate in a categorical perception task and harder to decode from patterns of fMRI activity in the hippocampus.

⭐️There is a linear relationship between the neural integration effect and the behavioral integration effect in PHC and in hippocampus.

These results indicate that increasing coactivation of objects in the visual cortex can drive their neural representations in the medial temporal lobe to integrate and that this can have behavioral consequences for perception.


12min for project2
Motivate this study: model
In the previous project, real-time fMRI was used to manipulate representations in the human brain to study representational level non-monotonic plasticity hypothesis NMPH.  
Here, we aim to explore NMPH at different levels, including synaptic levels and representational levels using simulations.
NMPH is a multi-level theory, spanning from the synaptic level, behavioral level, and neural level.

⭐️Theoretically, at the synaptic level, the Non-Monotonic Plasticity Hypothesis (NMPH) indicates that the synaptic strength decreases when synaptic activity is at a moderate level, known as Long-Term Depression; synapses get strengthened at higher activity levels, known as Long-Term Potentiation.

⭐️At the level of behavior and neural data, human studies using EEG and fMRI have shown that NMPH can explain human behaviors. For example, in negative priming studies, moderate levels of distraction processing are associated with slower subsequent responses to stimuli, while higher and lower levels of distraction processing are not related.











Lastly, at the neural network level, NMPH offers a new perspective for understanding competition issues in recurrent neural networks. In such networks, feedback loops between layers can lead to complex dynamic behaviors. NMPH helps the network reach a stable state more efficiently by reducing such competition. Furthermore, by adjusting the excitability level within brain regions, such as by changing the amplitude of inhibitory neuronal oscillations, the effect of NMPH can be further optimized, thereby improving the accuracy and stability of memories.

At the same time, we have found similar algorithms in contrastive learning, such as the local aggregation algorithm, which involves a push-pull mechanism for distant neighbors, near neighbors, and background neighbors, equivalent to the action mechanism of NMPH at the neural network level. Meanwhile, many algorithms attempt to implement NMPH at the synaptic level, but there has not yet been a study that simultaneously investigates NMPH at different levels. In this research, we hope to systematically explore NMPH at various levels and attempt to understand how NMPH at different levels impacts the ultimate performance of algorithms.


⭐️We have already introduced the nonmonotonic plasticity hypothesis, which says that two representations would integrate when strongly co-activated, differentiate when moderately activated, and not change when minimally activated.
⭐️We found some equivalence in the local aggregation algorithm, which is an unsupervised learning algorithm successfully used in CNNs. For each red center dot, it tries to pull together close neighbors like these blue dots and push away background neighbors like these black dots. In theory, the local aggregation algorithm is equivalent to non-monotonic plasticity, where close neighbors integrate with red dots and background neighbors differentiate with center dot. Irrelevant grey neighbors do not change, just like NMPH’s left arm.
I have tried to directly implement close neighbor pulling and background neighbor pushing cannot show an integration effect effectively, only differentiation shows up. ⭐️

之前的项目通过实时fMRI去操纵人脑中的表征来研究非单调可塑性，这里我们希望去研究非单调可塑性在算法中的应用和不同层次， 包括突触层次和表征层次的适用情况。
理论上来说，在突触层次上，NMPH指出神经元之间的连接强度并非单一方向地随着使用频率增加而增强，而是呈现出非线性的变化模式。具体来说，当突触的活动处于适度水平时，突触连接可能会减弱，这是一种称为长时程抑制（LTD）的现象；而当活动水平更高时，则可能导致突触增强，即长时程增强（LTP）。

在行为层次和神经数据层面，人类研究使用EEG和fMRI显示，NMPH可以解释人类行为，例如在负性启动negative priming研究中，适度水平的分心处理与负性启动negative priming（对刺激的后续反应较慢）相关，而更高和更低水平的分心处理则没有关系。

最后，在神经网络层面，NMPH为我们提供了理解循环神经网络中竞争问题的新视角。在这类网络中，层间的反馈循环可能导致复杂的动态行为，NMPH通过减少这种竞争，帮助网络更加高效地达到稳定状态。此外，通过调整大脑区域内部的兴奋性水平，比如通过改变抑制性神经振荡的幅度，NMPH的作用可以被进一步优化，从而提高记忆的准确性和稳定性。

与此同时， 我们发现在对比学习中，也有类似的算法，比如local aggregation算法涉及到对于远邻居，近邻居和背景邻居的推拉方式等价于NMPH在神经网络层面的作用机制。
与此同时，我们也看到了很多在突触层面试图实现NMPH的算法， 但是目前还没有一个研究去同时研究不同层次的NMPH。
在这项研究中， 我们希望系统性地去探索不同层面的NMPH并且试图去理解究竟是不同层次的NMPH对于算法的最终性能的影响是如何的。
Translating representational level NMPH to the interaction between multiple representations, tells us that when the shared part of bed and chair are highly activated, then the shared memory is enhanced and thus the two memories will integrate ⭐️, and it becomes more difficult for the subject to distinguish between bed and chair. This is called the integration. 
When the shared areas were moderately activated⭐️, then the shared representation will be diminished ⭐️, and the two memory representations would be more separated from each other. The subject would be more capable to distinguish between them. This is called the differentiation.


There are a lot of implementations of synaptic level NMPH. Like ⭐️Oja’s rule, ⭐️SoftHebb network, ⭐️Krotov network, ⭐️emergent XCAL. They tried to use local synaptic level NMPH to learn a mature representation. 

⭐️There is also a vast field called contrastive learning in machine learning, which is equivalent to representational NMPH. They tried to use a representational level NMPH equivalent algorithm to learn mature representation. 
⭐️One example is called local aggregation, ⭐️it first converts each image into a dot in the embedding space, ⭐️ then for each red dot in the embedding space, it has some blue close neighbors and black background neighbors. Local aggregation algorithm pulls blue dots closer to the red dot, ⭐️exactly like the integration part of the NMPH curve, push away the black dots from the red dot, ⭐️exactly like the differentiation part of the NMPH curve. For these grey dots, they are not moved, ⭐️exactly like this part of the NMPH curve, where learning does not happen at all.


However, we still don’t have a simulation framework that can accomplish both of these two levels of NMPH, just like the human brain did. This is important because it is the parallel local synaptic NMPH that makes the human brain efficient in learning. It is the representational level NMPH that makes the brain able to minimize complex dynamic behaviors, reach a stable state more efficiently, and update the relative position of existing memories in the representational space as we experience the world in real time.



However, we still don’t have a simulation framework that can accomplish both of these two levels of NMPH, just like the human brain did. 
Simultaneously achieving these two levels is very important because 
⭐️it is the parallel local synaptic NMPH that makes the human brain efficient in learning. 
⭐️It is the representational level NMPH that ⭐️ improves the efficiency and accuracy of memory retrieval because appropriate differentiation or integration facilitates memory storage and recall by reducing unnecessary interference.


I first look at the existing frameworks to see if they are able to do this. 
⭐️I looked into the local aggregation model, which uses representational NMPH to learn the weights. 
⭐️We extracted its synaptic activation and synaptic change, then plotted a synaptic level NMPH and found that there is no obvious NMPH U-shaped pattern here.
⭐️I also looked into different implementations of synaptic level NMPH. 
⭐️I first looked at the SoftHebb network, which adjusts the synaptic weight based on a function of presynaptic neuron activity and post-synaptic neuron activity. It is a form of synaptic NMPH rule.
From this implementation of learning from the synaptic level, I designed a way to look at whether the representational level change obeys the NMPH rule.

I record the same set of testing probes ⭐️before and ⭐️after training the model with synaptic NMPH, Then I calculate the pairwise correlation similarity between each possible pair of testing probes. I use the ⭐️between-probe similarity before training as the representational co-activation or the ⭐️ representational level NMPH plot ⭐️ x-axis. The ⭐️between-probe similarity after training is calculated and the ⭐️ change in similarity before and after training is used as the representational change or the ⭐️ y-axis of the plot. If there is indeed a representational level NMPH, there should be a U-shaped curve in the representational level NMPH plot.

⭐️The result is that SoftHebb network is not showing representational level NMPH.

⭐️We also looked at Krotov Hopfield network, which use another linear wise NMPH form to train the network at the synaptic level, they also did not show representational level NMPH.

Finally, we looked into the emergent XCAL implementation. XCAL stands for “eXtended Contrastive Attractor Learning”.
XCAL has a supervised component and an unsupervised component. Here we focus on its unsupervised part since this is what is equivalent to NMPH.

⭐️BCM function is a form of NMPH curve with a floating threshold determined by 
⭐️XCAL is a linearized BCM (Elie Bienenstock, Leon Cooper, Paul Munro) function with a floating threshold which is determined by average activity levels of the synapse over a long time frame.

⭐️We examined an XCAL model that implements a one-hidden-layer neural network called v1rf. It has two input layers LGNon and LGNoff, feedforward projecting to the hidden V1 layer. The V1 layer has some circular excitatory and inhibitory connections.

Topographic organization of oriented edge detectors in the simulation of V1 neurons exposed to small windows of natural images (mountains trees etc). The neighborhood connectivity of neurons causes a topographic organization to develop.

Here are the testing stimuli, the right panel is the original testing stimuli. The left panel is the one I created.

⭐️I did the representational level NMPH analysis on this network and found that it indeed obeys the NMPH rule both at the synaptic level as well as the representational level.
⭐️I also did a 5-fold cross-validation and trained a cubic fit on the training set and predicted the testing set, and correlated the predicted data and real data. The correlation is significantly above zero, meaning that the data can be successfully fitted by a cubic fit.
⭐️I also did a sliding bin analysis and for the dots within each bin, I calculated the 5 to 95 percentile of the resampled mean distribution and plotted them here. This plot also tells us that the XCAL implementation satisfies both synaptic and representational level NMPH.





C:\GoGi\sims\ch6\v1rf\data\dataAnalysis.py   (ch6/v1rf/


data/dataAnalysis.py)
path_name = "/gpfs/milgram/scratch60/turk-browne/kp578/chanales/v1rf/record0305_probes14_100epoch.csv"
I inserted 9 full testing probe into the training process.
Here I am comparing the first time point and the 8th timepoining, meaning the full testing probe before the network is trained at all and the full testing probe after the network is fully trained.

This is an ongoing study, so the future directions for this study include ⭐️some ablation tests asking what is the key component for the representational NMPH. Hebbian/WTA/FFFB/within-layer-recurrence/between-layer-recurrence/sliding threshold?
⭐️Does XCAL show representational NMPH in multiple-layer networks?
⭐️Can we develop this representational NMPH as a constraint for better network training?

